
# ğŸ• Pizza Store Hygiene Violation Detection System

A real-time computer vision system designed to monitor hygiene compliance in a pizza store using microservices. It detects if an employee interacts with protein ingredients **without a scooper** and flags these interactions as violations.

---

## ğŸ”§ Architecture Overview

This project is built as a **microservices-based architecture** comprising:

| Component         | Description                                                                 |
|------------------|-----------------------------------------------------------------------------|
| ğŸ–¼ï¸ Frame Reader   | Reads video stream and publishes frames to RabbitMQ                        |
| ğŸ§  Detection Service | Tracks hands, scoopers, pizza interactions using YOLOv12 + logic            |
| ğŸŒ Streaming API  | Displays real-time annotated video and serves REST/WebSocket endpoints     |
| ğŸ’» Frontend UI    | Simple dashboard displaying video feed + violations in real time           |
| ğŸ“¦ Message Broker | RabbitMQ manages communication between services                            |

---

## ğŸ“ Folder Structure

```
PIZZA-VIOLATION-SYSTEM/
â”‚
â”œâ”€â”€ detection_service/        # Core logic: detection, violation logic, ROI handling
â”‚   â”œâ”€â”€ config.py
â”‚   â””â”€â”€ detect_violations.py
â”‚   
â”‚â”€â”€ frame_reader/             # Video frame publisher
â”‚   â””â”€â”€ frame_reader.py
â”‚
â”œâ”€â”€ models/                   # Trained YOLOv12 model weights
â”‚   â””â”€â”€ best.pt
â”‚
â”œâ”€â”€ results/                  # Output results
â”‚   â”œâ”€â”€ processed_video.mp4
â”‚   â””â”€â”€ violations/
â”‚       â”œâ”€â”€ violation_*.jpg
â”‚       â””â”€â”€ violations.json
â”‚
â”œâ”€â”€ samples/                  # Test videos
â”‚
â”œâ”€â”€ streaming_service/        # FastAPI server
â”‚   â”œâ”€â”€ app.py
â”‚   â””â”€â”€ templates/
â”‚       â””â”€â”€ index.html
â”‚
â”œâ”€â”€ utils/                    # Reusable utilities
â”‚   â”œâ”€â”€ helpers.py
â”‚   â””â”€â”€ virtual_id_tracker.py
â”‚
â”œâ”€â”€ yolov12/                  # YOLOv12 source (cloned from GitHub)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ pizza-final.ipynb         # Notebook used to train YOLOv12
â””â”€â”€ README.md
```

---

## ğŸ¥ Detection Logic

1. **Define ROIs** (`protein_1`, `protein_2`, etc.) for scooper-required zones.
2. For each frame:
   - Detect `Hand`, `Pizza`, and `Scooper` objects via YOLOv12.
   - Track hands using `VirtualIDTracker` for consistent identification.
   - If a **hand touches pizza without using a scooper** AND:
     - Entered ROI but didnâ€™t stay 11 seconds = ğŸš¨ **Violation**
     - OR used hand directly without scooper = ğŸš¨ **Violation**
   - Else if:
     - Hand stayed >11s without touching = âœ… **Cleaning**
     - Or touched pizza using scooper = âœ… **Safe**
3. Frame and violation details saved to `results/violations/`.

---

## ğŸ’» Frontend UI Features

- Displays live video feed (updated per frame)
- Shows `VirtualID` per hand
- Highlights ROI zones
- Alerts violation with frame snapshot + log
- Displays total violation count

---

## ğŸš€ Getting Started

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Configure RabbitMQ

Install RabbitMQ:
```bash
sudo apt update
sudo apt install rabbitmq-server
sudo systemctl start rabbitmq-server
sudo systemctl enable rabbitmq-server
sudo systemctl status rabbitmq-server
```

Or use Docker:
```bash
docker run -d --hostname rabbit --name rabbitmq \
  -p 5672:5672 -p 15672:15672 rabbitmq:3-management
```

RabbitMQ UI: [http://localhost:15672](http://localhost:15672) (user: `guest`, pass: `guest`)

### 3. Run Services (in 3 terminals)

**Terminal 1: Frame Publisher**
```bash
python detection_service/frame_reader.py
```

**Terminal 2: Detection & Violation Logic**
```bash
python detection_service/detect_violations.py
```

**Terminal 3: Web Streaming Service**
```bash
python streaming_service/app.py
```

Then open: [http://localhost:8000](http://localhost:8000)

---

## ğŸ§  Virtual ID Tracking

YOLOâ€™s `track_id` values can be unstable between frames. The `VirtualIDTracker` solves this by:
- Assigning stable virtual IDs based on object proximity
- Tracking object positions across frames
- Ensuring consistency for violation timing, scooper usage, and cleaning

This makes violation detection far more robust.

---

## ğŸ§ª Model Training (YOLOv12)

This project uses a custom-trained **YOLOv12** model to detect:

- `Hand`
- `Scooper`
- `Pizza`
- `Person` 

### ğŸ“Š Dataset

- Annotated via Roboflow:  
  ğŸ‘‰ [PizzaStore Dataset](https://app.roboflow.com/abdelrhman-medhat/pizzastore-qftv2/5)

### ğŸ§  Training Notebook

- Training performed using: `pizza-final.ipynb`
- Model base: YOLOv12 `yolov12m.pt`
- Training repo: [https://github.com/sunsmarterjie/yolov12](https://github.com/sunsmarterjie/yolov12)

### ğŸ‹ï¸â€â™‚ï¸ How to Retrain

1. Clone YOLOv12:
```bash
git clone https://github.com/sunsmarterjie/yolov12
cd yolov12
```

2. Export dataset from Roboflow as YOLOv5 format.

3. Train the model:
```bash
python train.py --data dataset.yaml --weights yolov12m.pt --cfg yolov12m.yaml --epochs 100 --img 640
```

4. Copy weights to project:
```bash
mv runs/train/exp/weights/best.pt ../models/best.pt
```

---

## ğŸ“¦ requirements.txt

```
ultralytics
opencv-python
pika
fastapi
uvicorn
numpy
```

---

## ğŸ“½ï¸ Output Logs

- Annotated video: `results/processed_video.mp4`
- Violations images: `results/violations/*.jpg`
- Log file: `results/violations/violations.json`

---

## ğŸ“¬ Contact & Submission

Developed by **Abdelrhman Medhat**  
Built for the **Eagle Vision** task submission: real-time microservices video analysis
